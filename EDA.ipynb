{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import multiprocessing as mp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Deep Learning Imports \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = os.path.exists(\"/kaggle/input\")\n",
    "if kaggle:\n",
    "    files = glob(\"../input/google-symptom-trends-as-of-october-1st-2022/datasets/20??_country_daily_20??_US_daily_symptoms_dataset.csv\")\n",
    "else:\n",
    "    files = glob(\"datasets/20??_country_daily_20??_US_daily_symptoms_dataset.csv\")\n",
    "    from EDAModule.RegionVis import generalRegionVisualiztion\n",
    "\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Stratification based on regions \n",
    "regions = df[\"sub_region_1\"].unique()\n",
    "regions = np.delete(regions, 0)\n",
    "dfs = [df[df[\"sub_region_1\"] == region] for region in regions]\n",
    "\n",
    "# Change data resolution to weekly\n",
    "def weekly(df):\n",
    "    # Convert date to pandas datetime object\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.resample(\"W\").mean()\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "dfsweekly = [weekly(df) for df in dfs]\n",
    "del dfs\n",
    "\n",
    "# Store the weekly dataframes to a pickle seperate pickle files\n",
    "for i, region in enumerate(regions):\n",
    "    try:\n",
    "        os.makedirs(f\"./datasets/weekly/{region}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    with open(f\"./datasets/weekly/{region}/dataset.pkl\", \"wb\") as f:\n",
    "        pkl.dump(dfsweekly[i], f)\n",
    "\n",
    "del dfsweekly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Case: Georgia \n",
    "\n",
    "We will be using Georgia as our case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./datasets/weekly/Georgia/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "symptoms = [col for col in df.columns if 'symptom' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a missing data Seaborn heatmap fon Georgia dataset\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motherfucker, the dataset looks clean and actually better. \n",
    "\n",
    "Except for the one symptom and a missing column. With the main dataset chosen from October we get a total of 192 weeks worth of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a distribution Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms], cmap=\"viridis\", cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a correlation Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms].corr(), cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    generalRegionVisualiztion(df, \"./datasets/weekly/Georgia/\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Analysis \n",
    "\n",
    "for region in regions:\n",
    "    df = pkl.load(open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\"))\n",
    "    print(f\"{region} has {df.isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data checkpoint\n",
    "\n",
    "Just saw the missing data heatmaps. Holy fuck, boy this is gonna be fun. \n",
    "\n",
    "States with the most missing data: Alaska, Delaware (I though Biden was from here), District of Columbia, Hawaii, Idaho, Maine, Mississippi, Montana, Nebraska, New Hampshire, New Mexico, North Dakota, Rhode Island, South Dakota, Utah, Vermont, West Virginia, Wyoming.\n",
    "\n",
    "States with the bearable missing data: Alabama, Arkansas, Connecticut, Iowa, Kansas, Kentucky, Lousiana, Minnesota, Missouri, Nevada, Oregon, Oklahoma, South Carolina, Wisconsin.\n",
    "\n",
    "With this consensus, the best way would be to train a model which have a better dataset like Florida, California, Georgia, Texas, New York and others which have a better dataset.\n",
    "\n",
    "The popularity of the term would be conserved even if the differential privacy threshold doesn't hold. \n",
    "Using STRATS to impute the missing data from the other states would be a good idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 0 to missing data\n",
    "\n",
    "# for region in regions:\n",
    "#     f = open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\")\n",
    "#     df = pkl.load(f)\n",
    "#     df = df.fillna(0)\n",
    "#     with open(f\"./datasets/weekly/{region}/dataset.pkl\", \"wb\") as f:\n",
    "#         pkl.dump(df, f)\n",
    "\n",
    "# Train a model for California, New York, Texas, Florida, Georgia, Illinois, Indiana, Maryland, Massachusetts, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, Washington, Wisconsin to impute missing data for Arkansas\n",
    "\n",
    "sym = 'symptom:Hemolysis'\n",
    "\n",
    "# Load training data \n",
    "trainingRegions = [\"California\", \"New York\", \"Texas\", \"Florida\", \"Georgia\", \"Illinois\", \"Indiana\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"New Jersey\", \"North Carolina\", \"Ohio\", \"Pennsylvania\", \"Tennessee\", \"Virginia\", \"Washington\", \"Wisconsin\"]\n",
    "trainingData = [pkl.load(open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\")).loc[:,['date',sym]].set_index('date') for region in trainingRegions]\n",
    "trainingData = pd.concat(trainingData, axis = 1, ignore_index = False)\n",
    "trainingData.columns = trainingRegions\n",
    "trainingData = trainingData.transpose()\n",
    "\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "testingData = pkl.load(open(f\"./datasets/weekly/Arkansas/dataset.pkl\", \"rb\")).loc[:,['date',sym]]\n",
    "masking_rate = testingData[sym].isnull().sum()/len(testingData)\n",
    "\n",
    "\n",
    "# Plot the training and testing data\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for region in trainingRegions:\n",
    "    sns.lineplot(data=trainingData.loc[region], label=region)\n",
    "# sns.lineplot(data=testingData, x=\"date\", y=sym, label=\"Testing Data\", hue = testingData[sym].isna().cumsum(), palette=[\"orange\"]*sum(testingData[sym].isna()) + [\"blue\"]*(len(testingData) - sum(testingData[sym].isna())), legend=False, markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making basic encoder and decoder model architecture \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4, latent_size):\n",
    "        super(AE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.hidden_size_3 = hidden_size_3\n",
    "        self.hidden_size_4 = hidden_size_4\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_1, self.hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_2, self.hidden_size_3),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.hidden_size_3, self.hidden_size_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_4, self.latent_size)\n",
    "        )\n",
    "        self.encoder_mu = nn.Linear(self.latent_size, self.latent_size)\n",
    "        self.encoder_logvar = nn.Linear(self.latent_size, self.latent_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.hidden_size_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_4, self.hidden_size_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_3, self.hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_2, self.hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_1, self.input_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def supervised_forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return mu, logvar, z\n",
    "    \n",
    "    def unsupervised_forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a simple Non-linear Autoencoder model\n",
    "input_size = trainingData.shape[1]\n",
    "\n",
    "class FClayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation = nn.ReLU()):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(input_size, output_size))\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(1, output_size))\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x,self.weight) + self.bias\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "# Layers \n",
    "\n",
    "hidden_layer_1 = FClayer(input_size = input_size, output_size = 128, activation = nn.ReLU())\n",
    "hidden_layer_2 = FClayer(input_size = 128, output_size = 64, activation = None)\n",
    "hidden_layer_3 = FClayer(input_size = 64, output_size = 32, activation = nn.ReLU())\n",
    "hidden_layer_4 = FClayer(input_size = 32, output_size = 16, activation = None)\n",
    "hidden_layer_5 = FClayer(input_size = 16, output_size = 8, activation = nn.ReLU())\n",
    "hidden_layer_6 = FClayer(input_size = 8, output_size = 4, activation = None)\n",
    "hidden_layer_7 = FClayer(input_size = 4, output_size = 8, activation = nn.ReLU())\n",
    "hidden_layer_8 = FClayer(input_size = 8, output_size = 16, activation = None)\n",
    "hidden_layer_9 = FClayer(input_size = 16, output_size = 32, activation = nn.ReLU())\n",
    "hidden_layer_10 = FClayer(input_size = 32, output_size = 64, activation = None)\n",
    "hidden_layer_11 = FClayer(input_size = 64, output_size = 128, activation = nn.ReLU())\n",
    "hidden_layer_12 = FClayer(input_size = 128, output_size = input_size, activation = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/shobrook/sequitur/blob/master/sequitur/models/lstm_ae.py\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [latent_size]\n",
    "        self.num_layers = len([input_size] + hidden_size + [latent_size]) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(layer_size[i], layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "            elif self.latent_activation and i == self.num_layers - 1:\n",
    "                return self.latent_activation(h_n).squeeze()\n",
    "        \n",
    "        return h_n.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [hidden_size[-1]]\n",
    "        self.num_layers = len(layer_size) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(layer_size[i], layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "        \n",
    "        self.dense_matrix = nn.Parameter(torch.rand((layer_size[-1], latent_size), dtype = torch.float), requires_grad = True)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.repeat(seq_len, 1).unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "        \n",
    "        return torch.mm(x.squeeze(), self.dense_matrix)\n",
    "\n",
    "        \n",
    "\n",
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size = [], hidden_activation = nn.Sigmoid(), latent_activation = nn.Tanh()):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        self.encoder = EncoderRNN(input_size, latent_size, hidden_size, hidden_activation, latent_activation)\n",
    "        self.decoder = DecoderRNN(latent_size, input_size, hidden_size[::-1], hidden_activation, latent_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[0]\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x, seq_len)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "\n",
    "input_size = trainingData.shape[1]\n",
    "hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4 = 128, 64, 32, 16\n",
    "latent_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# AElinear = AE(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4, latent_size).to(device)\n",
    "# AEFC = nn.Sequential(*[hidden_layer_1, hidden_layer_2, hidden_layer_3, hidden_layer_4, hidden_layer_5, hidden_layer_6, hidden_layer_7, hidden_layer_8, hidden_layer_9, hidden_layer_10, hidden_layer_10, hidden_layer_11, hidden_layer_12])\n",
    "AE_LSTM = LSTM_AE(input_size, latent_size, [hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4]).to(device)\n",
    "lr = 1e-3\n",
    "model = AE_LSTM.copy()\n",
    "criterion = nn.MSELoss(size_average = False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "num_epochs = 1000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    # Randomize the order of training data\n",
    "    random_order = np.random.permutation(trainingData.shape[0])\n",
    "    trainingData = trainingData.iloc[random_order]\n",
    "    # Reduces learning rate every 50 epochs\n",
    "    if not epoch % 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr * (0.993 ** epoch)\n",
    "    for region in trainingRegions:\n",
    "        data = torch.tensor(trainingData.loc[region].values).float()\n",
    "        data = data.to(device)\n",
    "        # Mask some values as nan\n",
    "        # mask = torch.rand_like(data) < masking_rate\n",
    "\n",
    "        # Mask some values as nan and store them in a new tensor\n",
    "        mask = torch.rand_like(data) < masking_rate\n",
    "        masked_data = data.clone()\n",
    "        masked_data[mask] = float('nan')\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(masked_data)\n",
    "        # output = model(mask.float() * data)\n",
    "        # output = model(data)\n",
    "\n",
    "        # Calculate the loss on the indexes where the values are not nan\n",
    "        loss = criterion(output[~mask], data[~mask])\n",
    "\n",
    "\n",
    "        loss = criterion(output, data)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch :{epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted and actual data\n",
    "model.eval()\n",
    "output = model(torch.tensor(testingData[sym].transpose().fillna(0).values).float().to(device))\n",
    "output = output.detach().numpy()\n",
    "\n",
    "output.size\n",
    "\n",
    "testingData['predicted'] = output\n",
    "\n",
    "# Substituting non nan values with actual values\n",
    "testingData.loc[~testingData[sym].isna(), 'predicted'] = testingData.loc[~testingData[sym].isna(), sym]\n",
    "\n",
    "testingData[sym].fillna(value = testingData[sym].mean(), inplace=True)\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.lineplot(data=testingData, x=\"date\", y=sym, label=\"Actual Data\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='predicted', label=\"Predicted Data\", legend=False, markers=True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "testingData = testingData.dropna()\n",
    "print(mean_squared_error(testingData[sym], testingData['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Learning Two Cents\n",
    "\n",
    "There is a lot of considerations we should keep in mind here.\n",
    "\n",
    "1. Lack of training data \n",
    "2. Handling of masking and what we need to optimize\n",
    "    > Elaboration on this: We need to predict missing data while my model is taking in entire masked data and then predicting actual data. \n",
    "    > Need help on this as to how this will work. \n",
    "3. I have used Linear Autoencoder and Fully Connected Autoencoder, which didn't work as expected. Fully Connected Autoencoder has better accuracy but than can be attributed to the fact that it had more layers. The gain in accuracy was marginal. \n",
    "4. Need to implement RNN techniques and most of it involves looking at the Attention Layer and understanding how it would help us. \n",
    "5. Masking is done on 0. We can preprocess the the data with the masking rate varying and then imputing it with the ffill and rfill methods. \n",
    "\n",
    "The imputation is 2 fronted problem now with the comparision and using LSTM to predict the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Visualization \n",
    "\n",
    "Lets see if the data joining worked and if we need to make adjustments for that in the early steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cough, fever, hypoxemia symptoms for Georgia dataset\n",
    "searchsymptoms = [\"cough\", \"fever\", \"hypoxemia\"]\n",
    "\n",
    "# Load Georgia dataset\n",
    "f = open(\"./datasets/weekly/Georgia/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "\n",
    "# Find columns which have cough, fever, and sore throat symptoms using regex search with ignoring case\n",
    "symptoms = [col for col in df.columns if any(re.search(search, col, re.IGNORECASE) for search in searchsymptoms)]\n",
    "\n",
    "# Plot the symptoms\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for symptom in symptoms:\n",
    "    plt.plot(df[\"date\"], df[symptom], label=symptom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Correlation Analysis\n",
    "\n",
    "### CCA\n",
    "\n",
    "CCA is a multivariate analysis technique that is used to find linear relationships between two sets of variables. It is a generalization of the Pearson correlation coefficient, which is used to find the linear relationship between two sets of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    df = pd.read_csv(\"../input/cdc-covid-tracker-dataset-for-us/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./datasets/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "\n",
    "# Stratify the data by state\n",
    "dfs = [df[df['state'] == region] for region in df['state'].unique()]\n",
    "\n",
    "for df in dfs:\n",
    "    df['date'] = pd.to_datetime(df['submission_date'])\n",
    "    # Aggregate the data by week\n",
    "    df = df.resample('W', on='date').sum()\n",
    "    # Select the columns we want \n",
    "    df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of the symptoms "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cse8803e')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52eb2e9aaa251fce06a7cb4186eb453f8b8c61cf125fe552a335749f5f35aeba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
