{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import multiprocessing as mp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = os.path.exists(\"/kaggle/input\")\n",
    "if kaggle:\n",
    "    files = glob(\"../input/google-symptom-trends-as-of-october-1st-2022/datasets/20??_country_daily_20??_US_daily_symptoms_dataset.csv\")\n",
    "else:\n",
    "    files = glob(\"datasets/20??_country_daily_20??_US_daily_symptoms_dataset.csv\")\n",
    "    from EDAModule.RegionVis import generalRegionVisualiztion\n",
    "\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Stratification based on regions \n",
    "regions = df[\"sub_region_1\"].unique()\n",
    "regions = np.delete(regions, 0)\n",
    "dfs = [df[df[\"sub_region_1\"] == region] for region in regions]\n",
    "\n",
    "# Change data resolution to weekly\n",
    "def weekly(df):\n",
    "    # Convert date to pandas datetime object\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.resample(\"W\").mean()\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "dfsweekly = [weekly(df) for df in dfs]\n",
    "del dfs\n",
    "\n",
    "# Store the weekly dataframes to a pickle seperate pickle files\n",
    "for i, region in enumerate(regions):\n",
    "    try:\n",
    "        os.makedirs(f\"./datasets/weekly/{region}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    with open(f\"./datasets/weekly/{region}/dataset.pkl\", \"wb\") as f:\n",
    "        pkl.dump(dfsweekly[i], f)\n",
    "\n",
    "del dfsweekly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Case: Georgia \n",
    "\n",
    "We will be using Georgia as our case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./datasets/weekly/Georgia/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "symptoms = [col for col in df.columns if 'symptom' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a missing data Seaborn heatmap fon Georgia dataset\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motherfucker, the dataset looks clean and actually better. \n",
    "\n",
    "Except for the one symptom and a missing column. With the main dataset chosen from October we get a total of 192 weeks worth of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a distribution Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms], cmap=\"viridis\", cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a correlation Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms].corr(), cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    generalRegionVisualiztion(df, \"./datasets/weekly/Georgia/\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Analysis \n",
    "\n",
    "for region in regions:\n",
    "    df = pkl.load(open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\"))\n",
    "    print(f\"{region} has {df.isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data checkpoint\n",
    "\n",
    "Just saw the missing data heatmaps. Holy fuck, boy this is gonna be fun. \n",
    "\n",
    "States with the most missing data: Alaska, Delaware (I though Biden was from here), District of Columbia, Hawaii, Idaho, Maine, Mississippi, Montana, Nebraska, New Hampshire, New Mexico, North Dakota, Rhode Island, South Dakota, Utah, Vermont, West Virginia, Wyoming.\n",
    "\n",
    "States with the bearable missing data: Alabama, Arkansas, Connecticut, Iowa, Kansas, Kentucky, Lousiana, Minnesota, Missouri, Nevada, Oregon, Oklahoma, South Carolina, Wisconsin.\n",
    "\n",
    "With this consensus, the best way would be to train a model which have a better dataset like Florida, California, Georgia, Texas, New York and others which have a better dataset.\n",
    "\n",
    "The popularity of the term would be conserved even if the differential privacy threshold doesn't hold. \n",
    "Using STRATS to impute the missing data from the other states would be a good idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 0 to missing data\n",
    "\n",
    "# for region in regions:\n",
    "#     f = open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\")\n",
    "#     df = pkl.load(f)\n",
    "#     df = df.fillna(0)\n",
    "#     with open(f\"./datasets/weekly/{region}/dataset.pkl\", \"wb\") as f:\n",
    "#         pkl.dump(df, f)\n",
    "\n",
    "# Train a model for California, New York, Texas, Florida, Georgia, Illinois, Indiana, Maryland, Massachusetts, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, Washington, Wisconsin to impute missing data for Arkansas\n",
    "\n",
    "sym = 'symptom:Hemolysis'\n",
    "\n",
    "# Load training data \n",
    "trainingRegions = [\"California\", \"New York\", \"Texas\", \"Florida\", \"Georgia\", \"Illinois\", \"Indiana\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"New Jersey\", \"North Carolina\", \"Ohio\", \"Pennsylvania\", \"Tennessee\", \"Virginia\", \"Washington\", \"Wisconsin\"]\n",
    "trainingData = [pkl.load(open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\")).loc[:,['date',sym]].set_index('date') for region in trainingRegions]\n",
    "trainingData = pd.concat(trainingData, axis = 1, ignore_index = False)\n",
    "trainingData.columns = trainingRegions\n",
    "trainingData = trainingData.transpose()\n",
    "\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "testingData = pkl.load(open(f\"./datasets/weekly/Arkansas/dataset.pkl\", \"rb\")).loc[:,['date',sym]]\n",
    "testingData = testingData.set_index('date')\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training and testing data\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for region in trainingRegions:\n",
    "    sns.lineplot(data=trainingData.loc[region], label=region)\n",
    "# sns.lineplot(data=testingData, x=\"date\", y=sym, label=\"Testing Data\", hue = testingData[sym].isna().cumsum(), palette=[\"orange\"]*sum(testingData[sym].isna()) + [\"blue\"]*(len(testingData) - sum(testingData[sym].isna())), legend=False, markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making basic encoder and decoder model architecture \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(AE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.latent_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "\n",
    "input_size = trainingData.shape[1]\n",
    "hidden_size = 8\n",
    "latent_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE(input_size, hidden_size, latent_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "masking_rate = 0.75\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for region in trainingRegions:\n",
    "        data = torch.tensor(trainingData.loc[region].values).float()\n",
    "        data = data.to(device)\n",
    "        # Mask some values as nan\n",
    "        mask = torch.rand_like(data) < masking_rate\n",
    "        # ===================forward=====================\n",
    "        output = model(mask.float() * data)\n",
    "        loss = criterion(output, data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f\"Epoch :{epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Visualization \n",
    "\n",
    "Lets see if the data joining worked and if we need to make adjustments for that in the early steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cough, fever, hypoxemia symptoms for Georgia dataset\n",
    "searchsymptoms = [\"cough\", \"fever\", \"hypoxemia\"]\n",
    "\n",
    "# Load Georgia dataset\n",
    "f = open(\"./datasets/weekly/Georgia/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "\n",
    "# Find columns which have cough, fever, and sore throat symptoms using regex search with ignoring case\n",
    "symptoms = [col for col in df.columns if any(re.search(search, col, re.IGNORECASE) for search in searchsymptoms)]\n",
    "\n",
    "# Plot the symptoms\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for symptom in symptoms:\n",
    "    plt.plot(df[\"date\"], df[symptom], label=symptom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Correlation Analysis\n",
    "\n",
    "### CCA\n",
    "\n",
    "CCA is a multivariate analysis technique that is used to find linear relationships between two sets of variables. It is a generalization of the Pearson correlation coefficient, which is used to find the linear relationship between two sets of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    df = pd.read_csv(\"../input/cdc-covid-tracker-dataset-for-us/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./datasets/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "\n",
    "# Stratify the data by state\n",
    "dfs = [df[df['state'] == region] for region in df['state'].unique()]\n",
    "\n",
    "for df in dfs:\n",
    "    df['date'] = pd.to_datetime(df['submission_date'])\n",
    "    # Aggregate the data by week\n",
    "    df = df.resample('W', on='date').sum()\n",
    "    # Select the columns we want \n",
    "    df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of the symptoms "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cse8803e')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52eb2e9aaa251fce06a7cb4186eb453f8b8c61cf125fe552a335749f5f35aeba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
