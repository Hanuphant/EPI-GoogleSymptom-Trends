{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import multiprocessing as mp\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from IPython.display import display\n",
    "\n",
    "# Deep Learning Imports \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = os.path.exists(\"/kaggle/input\")\n",
    "if kaggle:\n",
    "    files = glob(\"../input/google-symptom-trends-as-of-october-1st-2022/202?_country_weekly_202?_US_weekly_symptoms_dataset.csv\")\n",
    "else:\n",
    "    files = glob(\"datasets/202?_country_weekly_202?_US_weekly_symptoms_dataset.csv\")\n",
    "    from EDAModule.RegionVis import generalRegionVisualiztion\n",
    "\n",
    "dfs = [pd.read_csv(file) for file in sorted(files)]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Stratification based on regions \n",
    "regions = df[\"sub_region_1_code\"].unique()\n",
    "regions = np.delete(regions, 0)\n",
    "dfs = [df[df[\"sub_region_1_code\"] == region].drop(columns=['sub_region_2', 'sub_region_2_code']) for region in regions]\n",
    "\n",
    "# Store the weekly dataframes to a pickle seperate pickle files\n",
    "for i, region in enumerate(regions):\n",
    "    try:\n",
    "        os.makedirs(f\"./datasets/weekly/{region[3:]}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    with open(f\"./datasets/weekly/{region[3:]}/dataset.pkl\", \"wb\") as f:\n",
    "        pkl.dump(dfs[i], f)\n",
    "\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Case: Georgia \n",
    "\n",
    "We will be using Georgia as our case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./datasets/weekly/GA/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "symptoms = [col for col in df.columns if 'symptom' in col]\n",
    "# df['sub_region_1_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a missing data Seaborn heatmap fon Georgia dataset\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motherfucker, the dataset looks clean and actually better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a distribution Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms], cmap=\"viridis\", cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a correlation Seaborn heatmap for each symptom dataset for Georgia\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df[symptoms].corr(), cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    generalRegionVisualiztion(df, \"./datasets/weekly/GA/\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Analysis \n",
    "\n",
    "goodregions = []\n",
    "for region in regions:\n",
    "    df = pkl.load(open(f\"./datasets/weekly/{region[3:]}/dataset.pkl\", \"rb\"))\n",
    "    # print(f\"{region} has {df.isnull().sum().sum()} missing values\")\n",
    "\n",
    "    # Metric to assess the missing data\n",
    "    print(f\"{region[3:]} has {df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100}% missing values\")\n",
    "\n",
    "    # Get regions with good data\n",
    "    if df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100 <= 0.01:\n",
    "        goodregions.append(region[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Empirical Statistical Analysis on the missingness in Google Symptom Data\n",
    "\n",
    "# In each region get the number of missing values for each symptom\n",
    "\n",
    "missingnessdf = pd.DataFrame()\n",
    "for region in regions:\n",
    "    df = pkl.load(open(f\"./datasets/weekly/{region[3:]}/dataset.pkl\", \"rb\"))\n",
    "    df = df[symptoms]\n",
    "    missingnessdf.loc[region[3:], symptoms] = df.isnull().sum().values\n",
    "\n",
    "# Plot a heatmap of the missingness for each symptom in each region\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(missingnessdf, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data checkpoint\n",
    "\n",
    "Just saw the missing data heatmaps. Holy fuck, boy this is gonna be fun. There are missing data but not much. The missingness might have exagerrated by the resolution of daily datasets and might have increased as well. \n",
    "\n",
    "States with the most missing data: Alaska, Delaware, District of Columbia, Hawaii, Maine, Montana, New Hampshire, North Dakota, Rhode Island, South Dakota, Vermont, Wyoming.\n",
    "\n",
    "With this consensus, the best way would be to train a model which have a better dataset like Florida, California, Georgia, Texas, New York and others which have a better dataset.\n",
    "\n",
    "The popularity of the term would be conserved even if the differential privacy threshold doesn't hold. \n",
    "Using STRATS to impute the missing data from the other states would be a good idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = 'symptom:Hemolysis'\n",
    "\n",
    "# Load training data \n",
    "trainingRegions = goodregions\n",
    "trainingData = [pkl.load(open(f\"./datasets/weekly/{region}/dataset.pkl\", \"rb\")).loc[:,['date',sym]].set_index('date') for region in trainingRegions]\n",
    "trainingData = pd.concat(trainingData, axis = 1, ignore_index = False)\n",
    "trainingData.columns = trainingRegions\n",
    "trainingData = trainingData.transpose()\n",
    "\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "testingData = pkl.load(open(f\"./datasets/weekly/AR/dataset.pkl\", \"rb\")).loc[:,['date',sym]]\n",
    "masking_rate = testingData[sym].isnull().sum()/len(testingData)\n",
    "\n",
    "\n",
    "# Plot the training and testing data\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for region in trainingRegions:\n",
    "    sns.lineplot(data=trainingData.loc[region], label=region)\n",
    "# sns.lineplot(data=testingData, x=\"date\", y=sym, label=\"Testing Data\", hue = testingData[sym].isna().cumsum(), palette=[\"orange\"]*sum(testingData[sym].isna()) + [\"blue\"]*(len(testingData) - sum(testingData[sym].isna())), legend=False, markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making basic encoder and decoder model architecture \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4, latent_size):\n",
    "        super(AE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.hidden_size_3 = hidden_size_3\n",
    "        self.hidden_size_4 = hidden_size_4\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_1, self.hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_2, self.hidden_size_3),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.hidden_size_3, self.hidden_size_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_4, self.latent_size)\n",
    "        )\n",
    "        self.encoder_mu = nn.Linear(self.latent_size, self.latent_size)\n",
    "        self.encoder_logvar = nn.Linear(self.latent_size, self.latent_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.hidden_size_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_4, self.hidden_size_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_3, self.hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_2, self.hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size_1, self.input_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def supervised_forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return mu, logvar, z\n",
    "    \n",
    "    def unsupervised_forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a simple Non-linear Autoencoder model\n",
    "input_size = trainingData.shape[1]\n",
    "\n",
    "class FClayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation = nn.ReLU()):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(input_size, output_size))\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(1, output_size))\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x,self.weight) + self.bias\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "# Layers \n",
    "\n",
    "hidden_layer_1 = FClayer(input_size = input_size, output_size = 128, activation = nn.ReLU())\n",
    "hidden_layer_2 = FClayer(input_size = 128, output_size = 64, activation = None)\n",
    "hidden_layer_3 = FClayer(input_size = 64, output_size = 32, activation = nn.ReLU())\n",
    "hidden_layer_4 = FClayer(input_size = 32, output_size = 16, activation = None)\n",
    "hidden_layer_5 = FClayer(input_size = 16, output_size = 8, activation = nn.ReLU())\n",
    "hidden_layer_6 = FClayer(input_size = 8, output_size = 4, activation = None)\n",
    "hidden_layer_7 = FClayer(input_size = 4, output_size = 8, activation = nn.ReLU())\n",
    "hidden_layer_8 = FClayer(input_size = 8, output_size = 16, activation = None)\n",
    "hidden_layer_9 = FClayer(input_size = 16, output_size = 32, activation = nn.ReLU())\n",
    "hidden_layer_10 = FClayer(input_size = 32, output_size = 64, activation = None)\n",
    "hidden_layer_11 = FClayer(input_size = 64, output_size = 128, activation = nn.ReLU())\n",
    "hidden_layer_12 = FClayer(input_size = 128, output_size = input_size, activation = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/shobrook/sequitur/blob/master/sequitur/models/lstm_ae.py\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [latent_size]\n",
    "        self.num_layers = len(layer_size) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(input_size = layer_size[i], hidden_size = layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "            elif self.latent_activation and i == self.num_layers - 1:\n",
    "                return self.latent_activation(h_n).squeeze()\n",
    "        \n",
    "        return h_n.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.input_s = input_size\n",
    "        self.latent_s = latent_size\n",
    "        self.hidden_s = hidden_size\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [hidden_size[-1]]\n",
    "        self.num_layers = len(layer_size) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(layer_size[i], layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "        \n",
    "        self.dense_matrix = nn.Parameter(torch.rand((layer_size[-1], latent_size), dtype = torch.float), requires_grad = True)  # latent_size == 1 for now or can use softmax layer\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.repeat(seq_len, 1).unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "        \n",
    "        return torch.matmul(x.squeeze(), self.dense_matrix).squeeze()\n",
    "\n",
    "        \n",
    "\n",
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size = [], hidden_activation = nn.Sigmoid(), latent_activation = nn.Tanh()):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderRNN(input_size, latent_size, hidden_size, hidden_activation, latent_activation)\n",
    "        self.decoder = DecoderRNN(latent_size, input_size, hidden_size[::-1], hidden_activation, latent_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x, seq_len)\n",
    "        return x# Source: https://github.com/shobrook/sequitur/blob/master/sequitur/models/lstm_ae.py\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [latent_size]\n",
    "        self.num_layers = len(layer_size) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(input_size = layer_size[i], hidden_size = layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "            elif self.latent_activation and i == self.num_layers - 1:\n",
    "#                 print(f\"Shape of x in encoder : {x.shape} and shape of latent h_n : {h_n.squeeze().shape}\")\n",
    "                return self.latent_activation(h_n).squeeze()\n",
    "        return h_n.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size, hidden_activation, latent_activation):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_s = input_size\n",
    "        self.latent_s = latent_size\n",
    "        self.hidden_s = hidden_size\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.latent_activation = latent_activation\n",
    "\n",
    "        layer_size = [input_size] + hidden_size + [hidden_size[-1]]\n",
    "        self.num_layers = len(layer_size) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.LSTM(layer_size[i], layer_size[i+1], num_layers = 1, batch_first=True))\n",
    "#         print(f\"Layer Size : {layer_size}\")\n",
    "        self.dense_matrix = nn.Parameter(torch.rand((layer_size[-1], latent_size), dtype = torch.float), requires_grad = True)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.repeat(seq_len, 1).unsqueeze(0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, (h_n, c_n) = layer(x)\n",
    "\n",
    "            if self.hidden_activation and i < self.num_layers - 1:\n",
    "                x = self.hidden_activation(x)\n",
    "#         print(f\"Shape of x in decoder : {x.squeeze(0).shape} and shape of matrix : {self.dense_matrix.shape}\")\n",
    "        return torch.mm(x.squeeze(0), self.dense_matrix)\n",
    "\n",
    "        \n",
    "\n",
    "class LSTM_AES(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, hidden_size = [], hidden_activation = nn.Sigmoid(), latent_activation = nn.Tanh()):\n",
    "        super(LSTM_AES, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_size, latent_size, hidden_size, hidden_activation, latent_activation)\n",
    "        self.decoder = Decoder(latent_size, input_size, hidden_size[::-1], hidden_activation, latent_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         seq_len = x.shape[0]\n",
    "        seq_len = 1\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x, seq_len)\n",
    "#         print(f\"Shape of LSTM module : {x.shape}\")\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple recurrent neural network\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(self, input_size, hidden_size, num_layers, num_classes)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc =):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "\n",
    "input_size = trainingData.shape[1]\n",
    "hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4 = 128, 64, 32, 16\n",
    "latent_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AE(input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4, latent_size).to(device)\n",
    "# model = nn.Sequential(*[hidden_layer_1, hidden_layer_2, hidden_layer_3, hidden_layer_4, hidden_layer_5, hidden_layer_6, hidden_layer_7, hidden_layer_8, hidden_layer_9, hidden_layer_10, hidden_layer_10, hidden_layer_11, hidden_layer_12])\n",
    "model = LSTM_AES(input_size, latent_size, [hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4]).to(device)\n",
    "lr = 5e-5\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "num_epochs = 200\n",
    "\n",
    "min_loss = 100000\n",
    "dfloss = pd.DataFrame(columns = ['epoch', 'loss'])\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    # Randomize the order of training data\n",
    "    trainingRegions = np.random.permutation(trainingRegions)\n",
    "#     trainingData = trainingData.iloc[random_order]\n",
    "    # Reduces learning rate every 50 epochs\n",
    "    if not epoch % 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr * (0.993 ** epoch)\n",
    "    for region in trainingRegions:\n",
    "        data = torch.tensor(trainingData.loc[region].values).float()\n",
    "        data = data.to(device)\n",
    "        # Mask some values as nan\n",
    "        mask = torch.rand_like(data) < masking_rate\n",
    "\n",
    "#         # Mask some values as nan and store them in a new tensor\n",
    "#         mask = torch.rand_like(data) < masking_rate\n",
    "        masked_data = data.clone()\n",
    "        masked_data[mask] = float(0)\n",
    "                \n",
    "\n",
    "        # Forward pass\n",
    "        output = model(masked_data)\n",
    "#         output = model(mask.float() * data)\n",
    "        # output = model(data)\n",
    "\n",
    "        # Calculate the loss on the indexes where the values are not nan\n",
    "        loss = criterion(output, data)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if min_loss > loss.item():\n",
    "        min_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"./LSTM_AE_onlymask.pth\")\n",
    "        print('Epoch [{}/{}], Loss:{:.7f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    dfloss = dfloss.append({'epoch': epoch, 'loss': loss.item()}, ignore_index=True)\n",
    "\n",
    "ax, fig = plt.subplots(figsize = (20, 10))\n",
    "sns.lineplot(x = 'epoch', y = 'loss', data = dfloss)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing data\n",
    "testingData = pkl.load(open(f\"./datasets/weekly/AR/dataset.pkl\", \"rb\")).loc[:,['date',sym]]\n",
    "masking_rate = testingData[sym].isnull().sum()/len(testingData)\n",
    "\n",
    "# Plot the predicted and actual data\n",
    "model.load_state_dict(torch.load(\"./LSTM_AE.pth\"))\n",
    "model.eval()\n",
    "output = model(torch.tensor(testingData[sym].transpose().fillna(0).values).float().to(device))\n",
    "output = output.detach().numpy()\n",
    "\n",
    "output.size\n",
    "\n",
    "testingData['predicted'] = output.transpose()\n",
    "\n",
    "# Substituting non nan values with actual values\n",
    "# testingData.loc[~testingData[sym].isna(), 'predicted'] = testingData.loc[~testingData[sym].isna(), sym]\n",
    "\n",
    "# Testing the various metrics\n",
    "testingData['meanImpute'] = testingData[sym].fillna(value = testingData[sym].mean())\n",
    "testingData['medianImpute'] = testingData[sym].fillna(value = testingData[sym].median())\n",
    "testingData['forwardFill'] = testingData[sym].fillna(method = 'ffill')\n",
    "testingData['backwardFill'] = testingData[sym].fillna(method = 'bfill')\n",
    "testingData['interpolate'] = testingData[sym].interpolate(method = 'linear')\n",
    "\n",
    "# Rolling mean imputation\n",
    "testingData['rollingMeanImpute'] = testingData[sym].fillna(value = testingData[sym].rolling(5).mean())\n",
    "\n",
    "testingData[sym] = testingData[sym].interpolate(method = 'krogh')\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "sns.lineplot(data=testingData, x=\"date\", y=sym, label=\"Actual Data but krogh interpolated\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='predicted', label=\"Predicted Data\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='meanImpute', label=\"Mean Imputation\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='medianImpute', label=\"Median Imputation\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='forwardFill', label=\"Forward Fill\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='backwardFill', label=\"Backward Fill\", legend=False, markers=True)\n",
    "# sns.lineplot(data=testingData, x=\"date\", y='interpolate', label=\"Interpolation\", legend=False, markers=True)\n",
    "sns.lineplot(data=testingData, x=\"date\", y='rollingMeanImpute', label=\"Rolling Mean Imputation\", legend=False, markers=True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "testingData = testingData.dropna()\n",
    "print(mean_squared_error(testingData[sym], testingData['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Learning Two Cents\n",
    "\n",
    "There is a lot of considerations we should keep in mind here.\n",
    "\n",
    "1. Lack of training data \n",
    "2. Handling of masking and what we need to optimize\n",
    "    > Elaboration on this: We need to predict missing data while my model is taking in entire masked data and then predicting actual data. \n",
    "    > Need help on this as to how this will work. \n",
    "3. I have used Linear Autoencoder and Fully Connected Autoencoder, which didn't work as expected. Fully Connected Autoencoder has better accuracy but than can be attributed to the fact that it had more layers. The gain in accuracy was marginal. \n",
    "4. Need to implement RNN techniques and most of it involves looking at the Attention Layer and understanding how it would help us. \n",
    "5. Masking is done on 0. We can preprocess the the data with the masking rate varying and then imputing it with the ffill and rfill methods. \n",
    "\n",
    "The imputation is 2 fronted problem now with the comparision and using LSTM to predict the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Visualization \n",
    "\n",
    "Lets see if the data joining worked and if we need to make adjustments for that in the early steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cough, fever, hypoxemia symptoms for Georgia dataset\n",
    "searchsymptoms = [\"cough\", \"fever\", \"hypoxemia\"]\n",
    "\n",
    "# Load Georgia dataset\n",
    "f = open(\"./datasets/weekly/Georgia/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "\n",
    "# Find columns which have cough, fever, and sore throat symptoms using regex search with ignoring case\n",
    "symptoms = [col for col in df.columns if any(re.search(search, col, re.IGNORECASE) for search in searchsymptoms)]\n",
    "\n",
    "# Plot the symptoms\n",
    "ax, fig = plt.subplots(figsize=(20, 10))\n",
    "for symptom in symptoms:\n",
    "    plt.plot(df[\"date\"], df[symptom], label=symptom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Correlation Analysis\n",
    "\n",
    "### CCA\n",
    "\n",
    "CCA is a multivariate analysis technique that is used to find linear relationships between two sets of variables. It is a generalization of the Pearson correlation coefficient, which is used to find the linear relationship between two sets of variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    df = pd.read_csv(\"../input/cdc-covid-tracker-dataset-for-us/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./datasets/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "\n",
    "dfs = []\n",
    "# Stratify the data by state\n",
    "for region in df['state'].unique():\n",
    "    statedf = df[df['state'] == region]\n",
    "    statedf['date'] = pd.to_datetime(statedf['submission_date'])\n",
    "    statedf = statedf.drop(['submission_date'], axis=1)\n",
    "    statedf = statedf.resample('W-MON', on='date').sum()\n",
    "    statedf = statedf.reset_index()\n",
    "    # Get full \n",
    "    statedf['state'] = region\n",
    "    try:\n",
    "        pkl.dump(statedf, open(f\"./datasets/weekly/{region}/CDCdataset.pkl\", \"wb\"))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    dfs.append(statedf)\n",
    "\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Georgia \n",
    "\n",
    "f = open(f\"./datasets/weekly/GA/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "f = open(f\"./datasets/weekly/GA/CDCdataset.pkl\", \"rb\")\n",
    "df2 = pkl.load(f)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Merge the datasets on date\n",
    "df = df.merge(df2, on='date', how='inner')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Feature selection using correlation on the symptoms vs covid cases\n",
    "symptoms = [col for col in df.columns if 'symptom' in col]\n",
    "\n",
    "# Correlation analysis of the symptoms \n",
    "dfcorr = df[symptoms + ['new_case']].corr()\n",
    "dfcorrcases = dfcorr.loc['new_case']\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.lineplot(y = dfcorr.loc['new_case'].sort_values(), x = range(len(dfcorr.loc['new_case'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Pearson correlation coefficient\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate, correlation_lags\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "gransymsdf = pd.DataFrame({'symptom':[],'grangerCausalityPVal':[], 'mmcorrelation':[], 'time_lag':[], 'abscorrelation':[]})\n",
    "\n",
    "# Get the granger causality test\n",
    "for symptom in symptoms:\n",
    "    grangerdat = df[['new_case', symptom]]\n",
    "\n",
    "    # Standardize the data by columns\n",
    "    grangerdatstd = (grangerdat - grangerdat.mean()) / grangerdat.std()\n",
    "    \n",
    "    # Normalize the data \n",
    "    grangerdatmm = (grangerdat - grangerdat.min()) / (grangerdat.max() - grangerdat.min())\n",
    "\n",
    "    # Scipy cross correlation\n",
    "    corrstd = correlate(grangerdatstd[symptom], grangerdatstd['new_case']) \n",
    "    lags = correlation_lags(len(grangerdat[symptom]), len(grangerdat['new_case']))\n",
    "    corr = correlate(grangerdatmm[symptom], grangerdatmm['new_case'])\n",
    "     \n",
    "    for columns in grangerdat.columns:\n",
    "        grangerdat[columns] = (grangerdat[columns] - grangerdat[columns].mean()) / grangerdat[columns].std()\n",
    "        \n",
    "    # Granger Test\n",
    "    gran = grangercausalitytests(grangerdat[['new_case', symptom]], maxlag=2, verbose=False)\n",
    "    \n",
    "    gransymsdf = gransymsdf.append({'symptom':symptom,'grangerCausalityPVal':gran[1][0]['ssr_ftest'][1], 'mmcorrelation':np.max(corr), 'time_lag':time_lag, 'abscorrelation': np.max(corrstd)} , ignore_index = True)\n",
    "        \n",
    "gransymsdf.to_csv('featureselectionofsymptoms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granger's Causality Test Assumption: Treats all symptoms as independent variables and the disease as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sin wave \n",
    "radians = np.linspace(0, 4*np.pi, 400)\n",
    "rng = np.random.RandomState(12)\n",
    "sin = np.sin(radians) + 0.1 * rng.randn(len(radians))\n",
    "cos = np.cos(radians) + 0.1 * rng.randn(len(radians))\n",
    "cos2 = 2*np.cos(radians) + 0.1 * rng.randn(len(radians))\n",
    "\n",
    "# Min-max scaling \n",
    "sinmm = (sin - sin.min()) / (sin.max() - sin.min())\n",
    "cosmm = (cos - cos.min()) / (cos.max() - cos.min())\n",
    "cos2mm = (cos2 - cos2.min()) / (cos2.max() - cos2.min())\n",
    "\n",
    "# Standardized scaling \n",
    "sinss = (sin - sin.mean()) / sin.std()\n",
    "cosss = (cos - cos.mean()) / cos.std()\n",
    "cos2ss = (cos2 - cos2.mean()) / cos2.std()\n",
    "\n",
    "corr = correlate(sin, cos)/len(radians)\n",
    "corr2 = correlate(sin, cos2)/len(radians)\n",
    "corrmm = correlate(sinmm, cosmm)/len(radians)\n",
    "corr2mm = correlate(sinmm, cos2mm)/len(radians)\n",
    "corrss = correlate(sinss, cosss)/len(radians)\n",
    "corr2ss = correlate(sinss, cos2ss)/len(radians)\n",
    "lags = correlation_lags(len(sin), len(cos))\n",
    "\n",
    "time_delay = lags[np.argmax(corr)]\n",
    "time_delay2 = lags[np.argmax(corr2)]\n",
    "time_delaymm = lags[np.argmax(corrmm)]\n",
    "time_delay2mm = lags[np.argmax(corr2mm)]\n",
    "time_delayss = lags[np.argmax(corrss)]\n",
    "time_delay2ss = lags[np.argmax(corr2ss)]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(radians, sin, label='sin')\n",
    "plt.plot(radians, cos, label='cos')\n",
    "plt.axvline(radians[time_delay], color='red', label='time delay raw')\n",
    "plt.axvline(radians[time_delay2], color='green', label='time delay raw 2')\n",
    "plt.axvline(radians[time_delaymm], color='orange', label='time delay minmax')\n",
    "plt.axvline(radians[time_delay2mm], color='purple', label='time delay minmax 2')\n",
    "plt.axvline(radians[time_delayss], color='blue', label='time delay standardized')\n",
    "plt.axvline(radians[time_delay2ss], color='black', label='time delay standardized 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot cross correlation\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(lags, corr)\n",
    "plt.plot(lags, corrmm)\n",
    "plt.plot(lags, corrss)\n",
    "plt.legend(['raw', 'minmax', 'standardized'])\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('correlation')\n",
    "plt.show()\n",
    "\n",
    "print(f'Time delay: {time_delay} \\t Correlation: {np.max(corr), np.max(corr2)} \\t Standardized: {np.max(corrss), np.max(corr2ss)} \\t Min-Max: {np.max(corrmm), np.max(corr2mm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 20 symptoms with highest absolute correlation\n",
    "corrsyms = gransymsdf.sort_values(by='abscorrelation', ascending=False).head(20)['symptom'].values\n",
    "mmcorrsyms = gransymsdf.sort_values(by='mmcorrelation', ascending=False).head(20)['symptom'].values\n",
    "\n",
    "# Extract top 20 symptoms with highest granger causality\n",
    "gransyms = gransymsdf.sort_values(by='grangerCausalityPVal', ascending=True).head(20)['symptom'].values\n",
    "\n",
    "# Extract top 20 symptoms with highest leading time lag\n",
    "gransymsdf['magdelay'] = gransyms['time_lag'].abs()\n",
    "lagsyms = gransymsdf.sort_values(by = 'magdelay', ascending=True).head(20)['symptom'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature Selection using Random Forest Regressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rng = np.random.RandomState(789)\n",
    "\n",
    "# Features\n",
    "features = [col for col in df.columns if 'symptom' in col]\n",
    "\n",
    "# Target\n",
    "target = 'new_case'\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=rng)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=rng)\n",
    "\n",
    "# Random Hyperparameter Grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "max_features = ['auto']\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=1, random_state=rng, n_jobs = -1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def eval(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    rmse = mean_squared_error(test_labels, predictions, squared=False)\n",
    "    print('Model Performance (RMSE) {}'.format(rmse))\n",
    "    return rmse\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = rng)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = eval(base_model, X_test, y_test)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = eval(best_random, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (-random_accuracy + base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [None, 50, 60, 70],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [3, 5, 7],\n",
    "    'n_estimators': [400, 500, 600, 700, 800]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = eval(best_grid, X_test, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (-grid_accuracy + base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Feature Selection using RFE-SVM and RFE-Linear Regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rng = np.random.RandomState(789)\n",
    "\n",
    "# Get the features\n",
    "features = [col for col in df.columns if 'symptom' in col]\n",
    "\n",
    "# Get the target\n",
    "target = 'new_case'\n",
    "\n",
    "# Get the features and target\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=rng)\n",
    "\n",
    "# RFECV to get the total number of features\n",
    "rfecv = RFECV(estimator=grid_search.best_estimator_, cv=5, verbose = 1, n_jobs = -1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the RFECV\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Get the number of features\n",
    "print(f'Optimal number of features: {rfecv.n_features_}')\n",
    "\n",
    "# Plot the number of features vs. cross-validation scores\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.show()\n",
    "\n",
    "# Get the selected features\n",
    "rfsyms = df[features].columns[rfecv.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting Effect of the Feature Selected Symptoms on the Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Study Results\n",
    "\n",
    "# DARTS Implementation\n",
    "try:\n",
    "    from darts import TimeSeries\n",
    "except ModuleNotFoundError:\n",
    "    !pip install darts\n",
    "    from darts import TimeSeries\n",
    "from darts.models.forecasting.nbeats import NBEATSModel\n",
    "from darts.metrics import mape\n",
    "\n",
    "# For Georgia \n",
    "\n",
    "f = open(f\"./datasets/weekly/GA/dataset.pkl\", \"rb\")\n",
    "df = pkl.load(f)\n",
    "f = open(f\"./datasets/weekly/GA/CDCdataset.pkl\", \"rb\")\n",
    "df2 = pkl.load(f)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Merge the datasets on date\n",
    "df = df.merge(df2, on='date', how='inner')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Get the time series\n",
    "tsy = TimeSeries.from_dataframe(df, 'date', target)\n",
    "trainy = tsy[:-20]\n",
    "\n",
    "def feature_selection_eval(features):\n",
    "    tsx = TimeSeries.from_dataframe(df, 'date', features)\n",
    "\n",
    "    # Get the training and testing data\n",
    "    trainy = tsy[:-20]\n",
    "    trainx = tsx[:-20]\n",
    "    testy = tsy[-20:-10]\n",
    "    testx = tsx[-20:-10]\n",
    "\n",
    "    # Get the model\n",
    "    model = NBEATSModel(input_chunk_length=20, output_chunk_length=10, n_epochs=100, random_state = 15)\n",
    "    model.fit(trainy, past_covariates=trainx, val_series=testy, val_past_covariates=testx ,epochs = 100)\n",
    "    predicted = model.predict(10)\n",
    "    \n",
    "    del model\n",
    "\n",
    "    return np.array(predicted.values()), mape(predicted, testy)\n",
    "\n",
    "# Get the features\n",
    "\n",
    "allsyms = [col for col in df.columns if 'symptom' in col]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(np.array(tsy.values()), label='actual')\n",
    "meths = {'pedicted_all':allsyms, 'predicted_granger':gransyms, 'predicted_pearson':pearsyms, 'predicted_rfe':svmsyms}\n",
    "for key in meths.keys():\n",
    "    pred, mapeval = feature_selection_eval(meths[key])\n",
    "    print(mapeval)\n",
    "    plt.plot(np.concatenate((np.array(trainy.values()),pred), axis = 0), label=key)\n",
    "plt.xlabel('Weeks')\n",
    "plt.ylabel('New Cases')\n",
    "plt.savefig('feature_selection.png', dpi=300)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Cents after Feature Selection\n",
    "\n",
    "From the graphs it's clear symptoms extracted based solely on the correlation do not warrant a better forecasting model. RFE and Granger prove to be more stable. \n",
    "\n",
    "With regards to the forecasting and based on the feature selection metric we will check the symptoms of nearby states to Georgia. \n",
    "\n",
    "Based on the map of Georgia, \n",
    "\n",
    "the 1-step neighbors are: Alabama, Florida, South Carolina, Tennessee and North Carolina\n",
    "\n",
    "the 2-step neighbors are: Arkansas, Kentucky, Mississippi, Missouri and Virginia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA1neighbors = ['AL', 'FL', 'NC', 'SC', 'TN']\n",
    "GA2neighbors = ['AR', 'KY', 'MS', 'MO', 'VA']\n",
    "\n",
    "# Find the Granger Causality symptoms for the neighbors \n",
    "def get_granger_symptoms(state):\n",
    "    # Load the data\n",
    "    f = open(f\"./datasets/weekly/{state}/dataset.pkl\", \"rb\")\n",
    "    df = pkl.load(f)\n",
    "    f = open(f\"./datasets/weekly/{state}/CDCdataset.pkl\", \"rb\")\n",
    "    df2 = pkl.load(f)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Merge the datasets on date\n",
    "    df = df.merge(df2, on='date', how='inner')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Get the symptoms\n",
    "    symptoms = [col for col in df.columns if 'symptom' in col]\n",
    "\n",
    "    # Get the Granger Causality\n",
    "    gransyms = []\n",
    "    corrsyms = []\n",
    "    for symptom in symptoms:\n",
    "        # Normalize \n",
    "        grangerdat = df[['new_case', symptom]]\n",
    "        for columns in grangerdat.columns:\n",
    "            grangerdat[columns] = (grangerdat[columns] - grangerdat[columns].mean()) / grangerdat[columns].std()\n",
    "\n",
    "        # Granger Test\n",
    "        try:\n",
    "            gran = grangercausalitytests(grangerdat[['new_case', symptom]], maxlag=2, verbose=False)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Correlation Test\n",
    "        corr, _ = pearsonr(grangerdat[symptom], grangerdat['new_case'])\n",
    "\n",
    "        if gran[1][0]['ssr_ftest'][1] < 0.001:\n",
    "            gransyms.append(symptom)\n",
    "\n",
    "        if corr > 0.2:\n",
    "            corrsyms.append(symptom)\n",
    "    \n",
    "    return gransyms, corrsyms\n",
    "\n",
    "# Get the Granger Causality Symptoms for Georgia\n",
    "GAgranger, GAcorr = get_granger_symptoms('GA')\n",
    "\n",
    "GAgrandf = pd.DataFrame({'state':'GA', 'granger_symptoms_shared':len(GAgranger), 'correlation_symptoms_shared':len(GAcorr)}, index=[0])\n",
    "\n",
    "# Get the Granger Causality Symptoms for neighbors and find the shared symptoms\n",
    "for state in GA1neighbors:\n",
    "    grangersyms, correlationsyms = get_granger_symptoms(state)\n",
    "    GAgrandf = GAgrandf.append({'state':state, 'granger_symptoms_shared':len(set(GAgranger).intersection(set(grangersyms))), 'correlation_symptoms_shared':len(set(GAcorr).intersection(set(correlationsyms))) }, ignore_index=True)\n",
    "\n",
    "for state in GA2neighbors:\n",
    "    grangersyms, correlationsyms = get_granger_symptoms(state)\n",
    "    GAgrandf = GAgrandf.append({'state':state, 'granger_symptoms_shared':len(set(GAgranger).intersection(set(grangersyms))), 'correlation_symptoms_shared':len(set(GAcorr).intersection(set(correlationsyms))) }, ignore_index=True)\n",
    "\n",
    "# Plot the results on US map using plotly\n",
    "import plotly.express as px\n",
    "fig = px.choropleth(GAgrandf, locations=\"state\",locationmode=\"USA-states\", color=\"granger_symptoms_shared\", scope=\"usa\", color_continuous_scale=px.colors.sequential.Blues)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(GAgrandf, locations=\"state\",locationmode=\"USA-states\", color=\"correlation_symptoms_shared\", scope=\"usa\", color_continuous_scale=px.colors.sequential.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cse8803e')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52eb2e9aaa251fce06a7cb4186eb453f8b8c61cf125fe552a335749f5f35aeba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
